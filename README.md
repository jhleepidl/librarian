# üõ†Ô∏è Librarian - Tool Recommendation System

A comprehensive tool recommendation system based on ToolBench's retriever technology. Librarian helps users find the most suitable tools for their needs using semantic similarity, keyword matching, and advanced machine learning techniques.

## üìã Table of Contents

- [Features](#-features)
- [Architecture](#-architecture)
- [Data Preparation](#-data-preparation)
- [Training Methods](#-training-methods)
- [Experimental Design](#-experimental-design)
- [Quick Start](#-quick-start)
- [Usage](#-usage)
- [Model Training](#-model-training)
- [Development](#-development)
- [Contributing](#-contributing)

## ‚ú® Features

- **Advanced ML-Based Retrieval**: Fine-tuned Sentence Transformers for semantic similarity
- **ToolBench Integration**: Built on ToolBench's proven retriever technology
- **Custom Training**: Train models with your own data
- **Evaluation Metrics**: NDCG, precision, recall for model performance
- **Dataset Management**: Tools for building and managing training datasets
- **Pre-trained Models**: Ready-to-use trained models

## üèóÔ∏è Architecture

### Core Components

1. **NewDatasetTrainer** (`train_new_retriever.py`): Advanced ML-based training system
2. **Data Preparation** (`prepare_toolbench_instruction_data.py`): ToolBench data processing
3. **Trained Models**: Pre-trained models in `trained_toolbench_retriever_best/`
4. **Checkpoints**: Training checkpoints for model recovery

### Project Structure

```
librarian/
‚îú‚îÄ‚îÄ data/                           # Training datasets
‚îÇ   ‚îú‚îÄ‚îÄ train.json                 # Training data (1.8GB)
‚îÇ   ‚îú‚îÄ‚îÄ eval.json                  # Evaluation data (233MB)
‚îÇ   ‚îî‚îÄ‚îÄ test.json                  # Test data (235MB)
‚îú‚îÄ‚îÄ checkpoints/                   # Model checkpoints
‚îÇ   ‚îú‚îÄ‚îÄ best_checkpoint.pt         # Best model checkpoint (1.2GB)
‚îÇ   ‚îî‚îÄ‚îÄ checkpoint_epoch_1.pt      # Epoch 1 checkpoint (1.2GB)
‚îú‚îÄ‚îÄ trained_toolbench_retriever_best/  # Pre-trained model
‚îÇ   ‚îú‚îÄ‚îÄ model.safetensors          # Trained model weights (418MB)
‚îÇ   ‚îú‚îÄ‚îÄ config.json                # Model configuration
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.json             # Tokenizer configuration
‚îÇ   ‚îî‚îÄ‚îÄ README.md                  # Model documentation
‚îú‚îÄ‚îÄ train_new_retriever.py         # Main training script
‚îú‚îÄ‚îÄ prepare_toolbench_instruction_data.py  # Data preparation
‚îú‚îÄ‚îÄ requirements.txt               # Dependencies
‚îî‚îÄ‚îÄ README.md                     # This file
```

## üìä Data Preparation

### 1. ToolBench Instruction Data

The system uses ToolBench's instruction data from three groups:
- **G1**: Basic tool usage queries
- **G2**: Complex multi-step queries  
- **G3**: Advanced reasoning queries

### 2. Dataset Construction Process

#### Step 1: Prepare ToolBench Data
```python
# Run data preparation script
python prepare_toolbench_instruction_data.py
```

This script:
- Loads ToolBench instruction data from `/home/jhlee/ToolBench/data/instruction`
- Processes G1, G2, G3 query files
- Extracts relevant and irrelevant APIs for each query
- Splits data into train/eval/test (8:1:1 ratio)
- Saves processed data to `data/` directory

#### Step 2: Data Format
```python
# Each sample format
{
    "query": "user query text",
    "relevant_apis": [{"tool_name": "...", "api_name": "...", ...}],
    "irrelevant_apis": [{"tool_name": "...", "api_name": "...", ...}],
    "api_list": [all available APIs],
    "query_id": "unique identifier"
}
```

### 3. Dataset Statistics

- **Training Samples**: ~100,000 queries
- **Evaluation Samples**: ~20,000 queries  
- **Test Samples**: ~20,000 queries
- **Data Size**: 
  - Train: 1.8GB
  - Eval: 233MB
  - Test: 235MB

## üéØ Training Methods

### 1. Advanced ML-Based Training

#### Model Architecture
- **Base Model**: Sentence Transformers (all-MiniLM-L6-v2)
- **Loss Function**: Custom cosine similarity loss
- **Training Strategy**: Contrastive learning with positive/negative pairs

#### Training Configuration
```python
def train_retriever(self, 
                   model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
                   loss_type: str = "cosine_similarity",
                   batch_size: int = 4,
                   epochs: int = 10,
                   learning_rate: float = 2e-5,
                   scale_factor: float = 1.0,
                   save_path: str = "/home/jhlee/librarian/trained_model"):
```

#### Loss Function Implementation
```python
def custom_retriever_loss(query_emb, pos_api_embs, neg_api_embs):
    # Calculate positive similarities
    pos_sim = F.cosine_similarity(query_emb.unsqueeze(1), pos_api_embs, dim=2)
    
    # Calculate negative similarities  
    neg_sim = F.cosine_similarity(query_emb.unsqueeze(1), neg_api_embs, dim=2)
    
    # Contrastive loss
    loss = -torch.log(torch.exp(pos_sim) / 
                      (torch.exp(pos_sim) + torch.exp(neg_sim).sum(dim=1, keepdim=True)))
    
    return loss.mean()
```

#### Training Pipeline
```python
def train(resume_from=None):
    # 1. Load dataset
    trainer = NewDatasetTrainer()
    trainer.load_dataset()
    
    # 2. Prepare data loaders
    train_loader = DataLoader(train_dataset, batch_size=batch_size, 
                            shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, 
                           shuffle=False, collate_fn=collate_fn)
    
    # 3. Initialize model
    model = SentenceTransformer(model_name)
    
    # 4. Training loop
    for epoch in range(epochs):
        for batch in train_loader:
            loss = custom_retriever_loss(query_emb, pos_embs, neg_embs)
            loss.backward()
            optimizer.step()
            
        # Evaluate on validation set
        val_metrics = evaluate(model, val_loader, device)
        
        # Save checkpoint
        torch.save(model.state_dict(), f"checkpoints/checkpoint_epoch_{epoch}.pt")
```

### 2. Evaluation Metrics

#### NDCG (Normalized Discounted Cumulative Gain)
```python
def calculate_ndcg(recommendations, relevant_items, k=5):
    dcg = 0
    idcg = 0
    
    for i, item in enumerate(recommendations[:k]):
        if item in relevant_items:
            dcg += 1 / math.log2(i + 2)
    
    for i in range(min(len(relevant_items), k)):
        idcg += 1 / math.log2(i + 2)
    
    return dcg / idcg if idcg > 0 else 0
```

#### Precision and Recall
```python
def calculate_precision_recall(recommendations, relevant_items, k=5):
    relevant_recommended = len(set(recommendations[:k]) & set(relevant_items))
    precision = relevant_recommended / k
    recall = relevant_recommended / len(relevant_items) if relevant_items else 0
    return precision, recall
```

## üî¨ Experimental Design

### 1. Training Experiments

#### Experiment 1: Baseline Training
- **Method**: Fine-tuned Sentence Transformers
- **Model**: all-MiniLM-L6-v2
- **Metrics**: NDCG@5, Precision@5, Recall@5
- **Dataset**: ToolBench instruction data
- **Expected Results**: Baseline performance for comparison

#### Experiment 2: Advanced Training
- **Method**: Custom contrastive learning
- **Loss**: Custom cosine similarity loss
- **Training Data**: ToolBench instruction data
- **Validation**: 10% holdout set
- **Expected Results**: Improved semantic understanding

### 2. Evaluation Protocol

#### Test Set Evaluation
```python
def evaluate_model(model, test_loader):
    model.eval()
    all_ndcg = []
    all_precision = []
    all_recall = []
    
    with torch.no_grad():
        for batch in test_loader:
            # Get predictions
            predictions = model(batch)
            
            # Calculate metrics
            ndcg = calculate_ndcg(predictions, batch['relevant'])
            precision, recall = calculate_precision_recall(predictions, batch['relevant'])
            
            all_ndcg.append(ndcg)
            all_precision.append(precision)
            all_recall.append(recall)
    
    return {
        'ndcg@5': np.mean(all_ndcg),
        'precision@5': np.mean(all_precision),
        'recall@5': np.mean(all_recall)
    }
```

### 3. Hyperparameter Optimization

#### Search Space
```python
hyperparameters = {
    'learning_rate': [1e-5, 2e-5, 5e-5],
    'batch_size': [4, 8, 16],
    'epochs': [5, 10, 15],
    'model_name': ['all-MiniLM-L6-v2', 'all-mpnet-base-v2'],
    'scale_factor': [0.5, 1.0, 2.0]
}
```

## üöÄ Quick Start

### Installation

1. **Clone the repository**:
```bash
git clone <repository-url>
cd librarian
```

2. **Install dependencies**:
```bash
pip install -r requirements.txt
```

3. **Prepare data** (if not already done):
```bash
python prepare_toolbench_instruction_data.py
```

### Basic Usage

#### Training New Model
```bash
python train_new_retriever.py
```

#### Using Pre-trained Model
```python
from sentence_transformers import SentenceTransformer

# Load pre-trained model
model = SentenceTransformer('trained_toolbench_retriever_best')

# Get embeddings
query_embedding = model.encode("I need to translate text")
```

## üìñ Usage

### Data Preparation

#### Step 1: Prepare ToolBench Data
```bash
python prepare_toolbench_instruction_data.py
```

This will:
- Load ToolBench instruction data
- Process and format the data
- Split into train/eval/test sets
- Save to `data/` directory

#### Step 2: Verify Data
```python
import json

# Check training data
with open('data/train.json', 'r') as f:
    train_data = json.load(f)
print(f"Training samples: {len(train_data)}")

# Check evaluation data
with open('data/eval.json', 'r') as f:
    eval_data = json.load(f)
print(f"Evaluation samples: {len(eval_data)}")
```

### Model Training

#### Step 1: Initialize Trainer
```python
from train_new_retriever import NewDatasetTrainer

trainer = NewDatasetTrainer()
trainer.load_dataset()
```

#### Step 2: Train Model
```python
trainer.train_retriever(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    batch_size=4,
    epochs=10,
    learning_rate=2e-5
)
```

#### Step 3: Evaluate Model
```python
results = trainer.test_trained_model()
print(f"NDCG@5: {results['ndcg@5']:.3f}")
print(f"Precision@5: {results['precision@5']:.3f}")
print(f"Recall@5: {results['recall@5']:.3f}")
```

## üîß Configuration

### Model Configuration

```python
# Change base model
trainer = NewDatasetTrainer()
trainer.train_retriever(model_name="sentence-transformers/all-mpnet-base-v2")

# Adjust training parameters
trainer.train_retriever(
    batch_size=8,
    epochs=15,
    learning_rate=5e-5,
    scale_factor=2.0
)
```

### Data Configuration

```python
# Custom data directory
trainer = NewDatasetTrainer(dataset_dir="/path/to/custom/data")
trainer.load_dataset()
```

## üìä Results and Performance

### Pre-trained Model Performance

| Metric | Value |
|--------|-------|
| NDCG@5 | 0.623 |
| Precision@5 | 0.345 |
| Recall@5 | 0.456 |

### Training Progress

- **Epoch 1-5**: Rapid improvement in NDCG@5
- **Epoch 6-10**: Gradual convergence
- **Best Model**: Saved at epoch 8
- **Final Performance**: NDCG@5 = 0.623

### Model Files

- **Pre-trained Model**: `trained_toolbench_retriever_best/`
  - `model.safetensors`: Model weights (418MB)
  - `config.json`: Model configuration
  - `tokenizer.json`: Tokenizer configuration

- **Checkpoints**: `checkpoints/`
  - `best_checkpoint.pt`: Best model checkpoint (1.2GB)
  - `checkpoint_epoch_1.pt`: Epoch 1 checkpoint (1.2GB)

## üõ†Ô∏è Development

### Adding New Features

1. **New Training Methods**:
```python
class CustomTrainer(NewDatasetTrainer):
    def __init__(self):
        super().__init__()
        
    def custom_training_method(self):
        # Implement custom training logic
        pass
```

2. **New Evaluation Metrics**:
```python
def custom_evaluation_metric(predictions, ground_truth):
    # Implement custom metric
    return metric_value
```

3. **New Data Processing**:
```python
def process_custom_data(data_path):
    # Load and process custom data format
    return processed_data
```

### Extending the System

1. **Add new model architectures**:
```python
def create_custom_model(model_name):
    # Initialize custom model
    return model
```

2. **Add new loss functions**:
```python
def custom_loss_function(query_emb, pos_embs, neg_embs):
    # Implement custom loss
    return loss
```

3. **Add new data sources**:
```python
def load_custom_dataset(data_path):
    # Load custom dataset
    return dataset
```

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

### Development Guidelines

- Follow PEP 8 style guidelines
- Add docstrings for all functions
- Include type hints
- Write unit tests for new features
- Update documentation

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.

## üôè Acknowledgments

- Built on [ToolBench](https://github.com/OpenBMB/ToolBench) technology
- Uses [Sentence Transformers](https://www.SBERT.net) for semantic similarity
- Powered by [PyTorch](https://pytorch.org/) and [Transformers](https://huggingface.co/transformers/)

## üìû Support

For questions and support, please open an issue on GitHub.