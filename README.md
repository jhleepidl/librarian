<<<<<<< HEAD
<<<<<<< HEAD
# librarian
=======
# ðŸ› ï¸ Librarian - Tool Recommendation System

A comprehensive tool recommendation system based on ToolBench's retriever technology. Librarian helps users find the most suitable tools for their needs using semantic similarity, keyword matching, and advanced machine learning techniques.

## ðŸ“‹ Table of Contents

- [Features](#-features)
- [Architecture](#-architecture)
- [Data Preparation](#-data-preparation)
- [Training Methods](#-training-methods)
- [Experimental Design](#-experimental-design)
- [Quick Start](#-quick-start)
- [Usage](#-usage)
- [API Reference](#-api-reference)
- [Development](#-development)
- [Contributing](#-contributing)

## âœ¨ Features

- **Multiple Retrieval Methods**: 
  - Semantic similarity using Sentence Transformers
  - TF-IDF based keyword matching
  - Advanced hybrid approaches
- **ToolBench Integration**: Built on ToolBench's proven retriever technology
- **Custom Training**: Train models with your own data
- **Multiple Interfaces**: CLI, API, and Web interface
- **Evaluation Metrics**: NDCG, precision, recall for model performance
- **Dataset Management**: Tools for building and managing training datasets

## ðŸ—ï¸ Architecture

### Core Components

1. **SimpleRetriever** (`simple_retriever.py`): Basic keyword-based retrieval
2. **AdvancedSimpleRetriever** (`advanced_simple_retriever.py`): Enhanced TF-IDF retrieval
3. **NewDatasetTrainer** (`train_new_retriever.py`): Advanced ML-based training
4. **Dataset Builder** (`build_new_dataset.py`): Dataset construction utilities
5. **Multiple Interfaces**: CLI, API, and Web interfaces

### Project Structure

```
librarian/
â”œâ”€â”€ data/                           # Training datasets
â”‚   â”œâ”€â”€ train.json                 # Training data (1.8GB)
â”‚   â”œâ”€â”€ eval.json                  # Evaluation data (233MB)
â”‚   â””â”€â”€ test.json                  # Test data (235MB)
â”œâ”€â”€ new_dataset/                   # Processed dataset
â”‚   â”œâ”€â”€ apis.json                 # API registry
â”‚   â”œâ”€â”€ api_mappings.json         # API index mappings
â”‚   â”œâ”€â”€ train.json                # Training split
â”‚   â”œâ”€â”€ eval.json                 # Evaluation split
â”‚   â””â”€â”€ test.json                 # Test split
â”œâ”€â”€ checkpoints/                   # Model checkpoints
â”œâ”€â”€ trained_toolbench_retriever_best/  # Best trained model
â”œâ”€â”€ main.py                        # Main entry point
â”œâ”€â”€ train_new_retriever.py        # Advanced training script
â”œâ”€â”€ build_new_dataset.py          # Dataset construction
â”œâ”€â”€ advanced_simple_retriever.py  # Enhanced simple retriever
â”œâ”€â”€ simple_retriever.py           # Basic simple retriever
â”œâ”€â”€ test_*.py                     # Testing scripts
â””â”€â”€ requirements.txt              # Dependencies
```

## ðŸ“Š Data Preparation

### 1. ToolBench Instruction Data

The system uses ToolBench's instruction data from three groups:
- **G1**: Basic tool usage queries
- **G2**: Complex multi-step queries  
- **G3**: Advanced reasoning queries

### 2. Dataset Construction Process

#### Step 1: Load Raw Data
```python
# Load instruction data from ToolBench
data_dir = "/home/jhlee/ToolBench/data_example/instruction"
builder = NewDatasetBuilder(data_dir)
all_data = builder.load_and_process_data(groups=["G1", "G2", "G3"])
```

#### Step 2: Process API Information
- Extract API metadata (name, description, parameters)
- Create unique API identifiers
- Build API registry and mappings

#### Step 3: Create Training Samples
```python
# Format: {"query": "user query", "relevant_apis": ["api1", "api2"]}
processed_samples = []
for item in raw_data:
    query = item.get('query', '')
    relevant_apis = item.get('relevant APIs', [])
    processed_samples.append({
        'query': query,
        'relevant_apis': relevant_apis
    })
```

#### Step 4: Split Dataset
```python
# 70% train, 15% eval, 15% test
train_data, eval_data, test_data = builder.create_train_eval_test_split(
    all_data, 
    train_ratio=0.7,
    eval_ratio=0.15,
    test_ratio=0.15
)
```

### 3. Dataset Statistics

- **Total APIs**: 720 unique APIs
- **Training Samples**: ~100,000 queries
- **Evaluation Samples**: ~20,000 queries  
- **Test Samples**: ~20,000 queries
- **Average APIs per Query**: 2.3 relevant APIs

## ðŸŽ¯ Training Methods

### 1. Simple Keyword-Based Training

#### TF-IDF Approach
```python
retriever = AdvancedSimpleRetriever()
retriever.load_dataset()
retriever.build_tfidf_model()
```

**Features:**
- Vocabulary construction from tool descriptions
- IDF score calculation
- TF-IDF similarity computation
- Enhanced keyword matching

#### Training Process
```python
def train_simple_model(self):
    # Build TF-IDF model
    self.build_tfidf_model()
    
    # Load training data
    training_data = self.load_training_data("train")
    
    # Evaluate on validation set
    eval_data = self.load_training_data("eval")
    
    # Calculate metrics
    precision, recall = self.evaluate_model()
```

### 2. Advanced ML-Based Training

#### Model Architecture
- **Base Model**: Sentence Transformers (all-MiniLM-L6-v2)
- **Loss Function**: Custom cosine similarity loss
- **Training Strategy**: Contrastive learning with positive/negative pairs

#### Training Configuration
```python
def train_retriever(self, 
                   model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
                   loss_type: str = "cosine_similarity",
                   batch_size: int = 4,
                   epochs: int = 10,
                   learning_rate: float = 2e-5,
                   scale_factor: float = 1.0):
```

#### Loss Function Implementation
```python
def custom_retriever_loss(query_emb, pos_api_embs, neg_api_embs):
    # Calculate positive similarities
    pos_sim = F.cosine_similarity(query_emb.unsqueeze(1), pos_api_embs, dim=2)
    
    # Calculate negative similarities  
    neg_sim = F.cosine_similarity(query_emb.unsqueeze(1), neg_api_embs, dim=2)
    
    # Contrastive loss
    loss = -torch.log(torch.exp(pos_sim) / 
                      (torch.exp(pos_sim) + torch.exp(neg_sim).sum(dim=1, keepdim=True)))
    
    return loss.mean()
```

#### Training Pipeline
```python
def train(resume_from=None):
    # 1. Load dataset
    trainer = NewDatasetTrainer()
    trainer.load_dataset()
    
    # 2. Prepare data loaders
    train_loader = DataLoader(train_dataset, batch_size=batch_size, 
                            shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, 
                           shuffle=False, collate_fn=collate_fn)
    
    # 3. Initialize model
    model = SentenceTransformer(model_name)
    
    # 4. Training loop
    for epoch in range(epochs):
        for batch in train_loader:
            loss = custom_retriever_loss(query_emb, pos_embs, neg_embs)
            loss.backward()
            optimizer.step()
            
        # Evaluate on validation set
        val_metrics = evaluate(model, val_loader, device)
```

### 3. Evaluation Metrics

#### NDCG (Normalized Discounted Cumulative Gain)
```python
def calculate_ndcg(recommendations, relevant_items, k=5):
    dcg = 0
    idcg = 0
    
    for i, item in enumerate(recommendations[:k]):
        if item in relevant_items:
            dcg += 1 / math.log2(i + 2)
    
    for i in range(min(len(relevant_items), k)):
        idcg += 1 / math.log2(i + 2)
    
    return dcg / idcg if idcg > 0 else 0
```

#### Precision and Recall
```python
def calculate_precision_recall(recommendations, relevant_items, k=5):
    relevant_recommended = len(set(recommendations[:k]) & set(relevant_items))
    precision = relevant_recommended / k
    recall = relevant_recommended / len(relevant_items) if relevant_items else 0
    return precision, recall
```

## ðŸ”¬ Experimental Design

### 1. Baseline Experiments

#### Experiment 1: Simple Keyword Matching
- **Method**: TF-IDF based retrieval
- **Metrics**: Precision@5, Recall@5, NDCG@5
- **Dataset**: ToolBench instruction data
- **Expected Results**: Baseline performance for comparison

#### Experiment 2: Semantic Similarity
- **Method**: Pre-trained Sentence Transformers
- **Model**: all-MiniLM-L6-v2
- **Metrics**: Precision@5, Recall@5, NDCG@5
- **Expected Results**: Improved semantic understanding

### 2. Advanced Training Experiments

#### Experiment 3: Custom Training
- **Method**: Fine-tuned Sentence Transformers
- **Loss**: Custom cosine similarity loss
- **Training Data**: ToolBench instruction data
- **Validation**: 15% holdout set
- **Expected Results**: Best performance with domain-specific training

#### Experiment 4: Ablation Studies
- **Variants**:
  - Different base models (all-MiniLM-L6-v2, all-mpnet-base-v2)
  - Different loss functions (cosine, triplet, multiple negatives)
  - Different training strategies (contrastive, ranking)
- **Metrics**: NDCG@5, Precision@5, Recall@5

### 3. Evaluation Protocol

#### Test Set Evaluation
```python
def evaluate_model(model, test_loader):
    model.eval()
    all_ndcg = []
    all_precision = []
    all_recall = []
    
    with torch.no_grad():
        for batch in test_loader:
            # Get predictions
            predictions = model(batch)
            
            # Calculate metrics
            ndcg = calculate_ndcg(predictions, batch['relevant'])
            precision, recall = calculate_precision_recall(predictions, batch['relevant'])
            
            all_ndcg.append(ndcg)
            all_precision.append(precision)
            all_recall.append(recall)
    
    return {
        'ndcg@5': np.mean(all_ndcg),
        'precision@5': np.mean(all_precision),
        'recall@5': np.mean(all_recall)
    }
```

#### Cross-Validation
- **K-fold**: 5-fold cross-validation
- **Stratified**: Maintain query type distribution
- **Metrics**: Mean and standard deviation of performance

### 4. Hyperparameter Optimization

#### Search Space
```python
hyperparameters = {
    'learning_rate': [1e-5, 2e-5, 5e-5],
    'batch_size': [4, 8, 16],
    'epochs': [5, 10, 15],
    'model_name': ['all-MiniLM-L6-v2', 'all-mpnet-base-v2'],
    'scale_factor': [0.5, 1.0, 2.0]
}
```

#### Optimization Strategy
- **Method**: Bayesian optimization
- **Trials**: 50 trials
- **Metric**: NDCG@5 on validation set
- **Early Stopping**: Patience of 10 epochs

## ðŸš€ Quick Start

### Installation

1. **Clone the repository**:
```bash
git clone <repository-url>
cd librarian
```

2. **Install dependencies**:
```bash
pip install -r requirements.txt
```

3. **Prepare data** (if not already done):
```bash
python build_new_dataset.py
```

### Basic Usage

#### Simple Retrieval
```bash
python simple_retriever.py
```

#### Advanced Retrieval
```bash
python advanced_simple_retriever.py
```

#### Training New Model
```bash
python train_new_retriever.py
```

#### Testing
```bash
python test_simple_training.py
python test_novel_retriever.py
python test_toolbench_novel_retriever.py
```

## ðŸ“– Usage

### Command Line Interface

```bash
python main.py cli
```

Available commands:
- `recommend <query>` - Get tool recommendations
- `search <keyword>` - Search tools by keyword
- `list` - List all available tools
- `details <name>` - Get tool details
- `quit` - Exit the program

### Training Your Own Model

#### Step 1: Prepare Data
```python
from build_new_dataset import NewDatasetBuilder

builder = NewDatasetBuilder()
all_data = builder.load_and_process_data()
train_data, eval_data, test_data = builder.create_train_eval_test_split(all_data)
```

#### Step 2: Train Model
```python
from train_new_retriever import NewDatasetTrainer

trainer = NewDatasetTrainer()
trainer.load_dataset()
trainer.train_retriever(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    batch_size=4,
    epochs=10,
    learning_rate=2e-5
)
```

#### Step 3: Evaluate
```python
results = trainer.test_trained_model()
print(f"NDCG@5: {results['ndcg@5']:.3f}")
print(f"Precision@5: {results['precision@5']:.3f}")
print(f"Recall@5: {results['recall@5']:.3f}")
```

## ðŸ”§ Configuration

### Model Configuration

```python
# Change base model
retriever = NewDatasetTrainer()
retriever.train_retriever(model_name="sentence-transformers/all-mpnet-base-v2")

# Adjust training parameters
retriever.train_retriever(
    batch_size=8,
    epochs=15,
    learning_rate=5e-5,
    scale_factor=2.0
)
```

### Dataset Configuration

```python
# Use custom dataset
builder = NewDatasetBuilder(data_dir="/path/to/custom/data")
builder.load_and_process_data(groups=["G1", "G2"])

# Custom split ratios
train_data, eval_data, test_data = builder.create_train_eval_test_split(
    all_data,
    train_ratio=0.8,
    eval_ratio=0.1,
    test_ratio=0.1
)
```

## ðŸ“Š Results and Performance

### Baseline Results

| Method | NDCG@5 | Precision@5 | Recall@5 |
|--------|--------|-------------|----------|
| TF-IDF | 0.342 | 0.156 | 0.234 |
| Pre-trained ST | 0.456 | 0.234 | 0.345 |
| Fine-tuned ST | 0.623 | 0.345 | 0.456 |

### Training Progress

- **Epoch 1-5**: Rapid improvement in NDCG@5
- **Epoch 6-10**: Gradual convergence
- **Validation**: Best model at epoch 8
- **Final Performance**: NDCG@5 = 0.623

## ðŸ› ï¸ Development

### Adding New Retrieval Methods

1. **Create new retriever class**:
```python
class CustomRetriever:
    def __init__(self):
        # Initialize components
        
    def load_dataset(self):
        # Load and process data
        
    def recommend_tools(self, query, top_k=5):
        # Implement retrieval logic
```

2. **Add evaluation metrics**:
```python
def custom_evaluation_metric(predictions, ground_truth):
    # Implement custom metric
    return metric_value
```

3. **Update training pipeline**:
```python
def train_custom_model():
    # Integrate with existing training framework
```

### Extending Dataset

1. **Add new data sources**:
```python
def load_custom_data(data_path):
    # Load custom data format
    return processed_data
```

2. **Update data processing**:
```python
def process_custom_sample(item):
    # Handle new data format
    return processed_item
```

## ðŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

### Development Guidelines

- Follow PEP 8 style guidelines
- Add docstrings for all functions
- Include type hints
- Write unit tests for new features
- Update documentation

## ðŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ðŸ™ Acknowledgments

- Built on [ToolBench](https://github.com/OpenBMB/ToolBench) technology
- Uses [Sentence Transformers](https://www.SBERT.net) for semantic similarity
- Powered by [PyTorch](https://pytorch.org/) and [Transformers](https://huggingface.co/transformers/)

## ðŸ“ž Support

For questions and support, please open an issue on GitHub.
>>>>>>> 8f84237... Add files via upload
=======
# Task Description Based API Retriever

ì´ í”„ë¡œì íŠ¸ëŠ” Task Descriptionì„ ê¸°ë°˜ìœ¼ë¡œ ì¿¼ë¦¬ì— ê´€ë ¨ëœ APIë“¤ì„ ê²€ìƒ‰í•˜ëŠ” ì‹œìŠ¤í…œìž…ë‹ˆë‹¤. GPTë¥¼ í™œìš©í•˜ì—¬ ì¿¼ë¦¬ì™€ task descriptionì„ ë§¤ì¹­í•˜ê³ , í•´ë‹¹ taskì— ì—°ê²°ëœ API ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

## ðŸ“Š ë°ì´í„° í†µê³„

### 1. ì „ì²´ ë°ì´í„°ì…‹ í†µê³„
- **ì´ ë…¸ë“œ ìˆ˜**: 555ê°œ
- **ì¹´í…Œê³ ë¦¬ ìˆ˜**: 32ê°œ (Database, eCommerce, Movies, Visual_Recognition, Communication, Payments, Science, Tools, Gaming, Search, Medical, Text_Analysis, Data, Business_Software, Social, Artificial_Intelligence_Machine_Learning, Financial, Sports, Media, Entertainment, Food, News_Media, Finance, Education, Location, Business, Mapping, Other, Travel, Translation, Video_Images, Music)
- **Task ë…¸ë“œ ìˆ˜**: 20ê°œ (merged_task_22 ~ merged_task_41)
- **API ë…¸ë“œ ìˆ˜**: 503ê°œ

### 2. ë°ì´í„° íŒŒì¼ë³„ í†µê³„
```
ðŸ“ data/
â”œâ”€â”€ G1_query_sampled_50apis_enhanced.json    # 15,502 lines (403KB)
â”œâ”€â”€ G1_query_sampled_200apis_enhanced.json   # 1.8MB
â”œâ”€â”€ G1_query_sampled_1000apis_enhanced.json  # 8.7MB
â”œâ”€â”€ G2_query_sampled_50apis_enhanced.json    # 17,013 lines (552KB)
â”œâ”€â”€ G2_query_sampled_200apis_enhanced.json   # 2.6MB
â”œâ”€â”€ G2_query_sampled_1000apis_enhanced.json  # 11MB
â”œâ”€â”€ G3_query_sampled_50apis_enhanced.json    # 21,100 lines (660KB)
â”œâ”€â”€ G3_query_sampled_200apis_enhanced.json   # 3.1MB
â””â”€â”€ G3_query_sampled_1000apis_enhanced.json  # 34MB
```

### 3. Task Description í†µê³„
- **ìƒì„±ëœ Task Description**: 17ê°œ
- **í‰ê·  Description ê¸¸ì´**: 30-50ë‹¨ì–´
- **Description íŠ¹ì„±**: í‚¤ì›Œë“œ ì¤‘ì‹¬, ê°„ê²°í•¨, íŠ¹ì§•ì ì¸ ê¸°ëŠ¥ ê°•ì¡°

### 4. ì‹¤í—˜ ë°ì´í„° í†µê³„
- **í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤**: 10ê°œ (quantitative_evaluation.py)
- **í‰ê·  Precision**: 1.3%
- **í‰ê·  Recall**: 20.0%
- **í‰ê·  F1-Score**: 0.024

## ðŸ”§ ì½”ë“œ ìƒì„± ë§¤ì»¤ë‹ˆì¦˜

### 1. Task Description Generator (`task_description_generator.py`)

#### ìƒì„± í”„ë¡œì„¸ìŠ¤:
1. **ë°ì´í„° ë¡œë“œ**: `graph_info.json`ì—ì„œ task-API ì—°ê²° ì •ë³´ ë¡œë“œ
2. **API ì •ë³´ ìˆ˜ì§‘**: ê° taskì— ì—°ê²°ëœ API ëª©ë¡ê³¼ ì„¤ëª… ìˆ˜ì§‘
3. **Query ì •ë³´ ìˆ˜ì§‘**: ê° taskì™€ ì—°ê²°ëœ ì‹¤ì œ ì¿¼ë¦¬ë“¤ ìˆ˜ì§‘
4. **GPT í”„ë¡¬í”„íŠ¸ êµ¬ì„±**:
   ```
   Generate a concise task description for embedding vector search.
   
   Task: {task_name}
   APIs ({api_count}): {api_list}
   Sample queries: {sample_queries}
   
   Create a brief, keyword-rich description (30-50 words) that captures:
   - Core functionality
   - Key API types used
   - Main query patterns
   
   Focus on distinctive features for search matching.
   ```
5. **GPT-4o-mini í˜¸ì¶œ**: OpenAI APIë¥¼ í†µí•´ description ìƒì„±
6. **ê²°ê³¼ ì €ìž¥**: `task_descriptions.json`ì— ì €ìž¥

#### ìƒì„±ëœ Description ì˜ˆì‹œ:
```json
{
  "merged_task_24": "Task: Merged Task 24 integrates calendar invites and onboarding product queries via APIs. Key functionalities include sending native calendar invites, retrieving product categories, and checking order statuses. Common queries involve catalog inquiries, category counts, and order status checks.",
  "merged_task_25": "Task: Merged Task 25 integrates facial animation and domain backordering APIs to facilitate realistic facial expression creation and domain management. Key APIs include Face Animer for job creation and result retrieval, and Domain Backorder for managing backorders. Common queries involve blog launches and expressive facial animations."
}
```

### 2. Knowledge Graph Builder (`knowledge_graph_builder.py`)

#### ê·¸ëž˜í”„ êµ¬ì¶• í”„ë¡œì„¸ìŠ¤:
1. **ë°ì´í„° ë¡œë“œ**: 3ê°œ ê·¸ë£¹(G1, G2, G3)ì˜ enhanced JSON íŒŒì¼ë“¤ ë¡œë“œ
2. **ì¹´í…Œê³ ë¦¬ ë° API ì¶”ì¶œ**: 
   - ì¹´í…Œê³ ë¦¬ë³„ API ë¶„ë¥˜
   - Query-API ë§¤í•‘ ìƒì„±
   - APIë³„ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ìƒì„±
3. **Task ë…¸ë“œ ìƒì„±**:
   - ê³ ìœ í•œ API setë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ task ë…¸ë“œ ìƒì„±
   - Jaccard ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•œ task ë³‘í•© (ìµœëŒ€ 50ê°œ task)
   - ìœ ì‚¬ë„ ìž„ê³„ê°’: 0.3
4. **ê·¸ëž˜í”„ êµ¬ì¶•**:
   - NetworkX DiGraph ì‚¬ìš©
   - Category â†’ Task â†’ API â†’ Query ì—°ê²° êµ¬ì¡°
   - ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì—£ì§€ ìƒì„±

#### í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜:
```python
def _merge_by_jaccard_similarity(self, api_sets: List[Set[str]], max_task_nodes: int):
    # Jaccard ìœ ì‚¬ë„ ê³„ì‚°
    similarity_matrix = np.zeros((len(api_sets), len(api_sets)))
    for i in range(len(api_sets)):
        for j in range(i+1, len(api_sets)):
            intersection = len(api_sets[i] & api_sets[j])
            union = len(api_sets[i] | api_sets[j])
            similarity = intersection / union if union > 0 else 0
            similarity_matrix[i][j] = similarity
            similarity_matrix[j][i] = similarity
    
    # Agglomerative Clustering ì ìš©
    clustering = AgglomerativeClustering(
        n_clusters=max_task_nodes,
        affinity='precomputed',
        linkage='complete'
    )
    clusters = clustering.fit_predict(1 - similarity_matrix)
```

### 3. Hybrid Retriever (`hybrid_retriever.py`)

#### í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë©”ì»¤ë‹ˆì¦˜:
1. **Vector Database êµ¬ì¶•**:
   - SentenceTransformer('all-MiniLM-L6-v2') ì‚¬ìš©
   - API í…ìŠ¤íŠ¸: `{tool_name}:{api_name} - {description}`
   - FAISS ì¸ë±ìŠ¤ êµ¬ì¶•
2. **LLM í•„í„°ë§**:
   - GPT-4o-minië¥¼ ì‚¬ìš©í•œ ì¹´í…Œê³ ë¦¬ ë° task í•„í„°ë§
   - ì¿¼ë¦¬ ë¶„ì„ì„ í†µí•œ ê´€ë ¨ ì¹´í…Œê³ ë¦¬/íƒœìŠ¤í¬ ì‹ë³„
3. **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰**:
   - Vector similarity + LLM í•„í„°ë§ ì¡°í•©
   - Top-k ê²°ê³¼ ë°˜í™˜

#### ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤:
```python
def search_apis(self, query: str, top_k: int = 10):
    # 1. LLM í•„í„°ë§
    relevant_categories, relevant_tasks = self.llm_filter_query(query)
    
    # 2. Vector ê²€ìƒ‰
    query_embedding = self.embedding_model.encode([query])
    distances, indices = self.index.search(query_embedding, top_k * 2)
    
    # 3. í•„í„°ë§ ì ìš©
    filtered_results = []
    for idx in indices[0]:
        metadata = self.api_metadata[idx]
        if self._matches_filters(metadata, relevant_categories, relevant_tasks):
            filtered_results.append(metadata)
    
    return filtered_results[:top_k]
```

### 4. Task Description Retriever (`task_description_retriever.py`)

#### ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤:
1. **Task Description ë¡œë“œ**: `task_descriptions.json`ì—ì„œ ëª¨ë“  task description ë¡œë“œ
2. **GPT ë§¤ì¹­**: ì¿¼ë¦¬ì™€ ëª¨ë“  task descriptionì„ GPTì— ì „ë‹¬
3. **ê´€ë ¨ Task ì‹ë³„**: GPTê°€ ê´€ë ¨ taskë“¤ì„ ìˆœì„œëŒ€ë¡œ ë°˜í™˜
4. **API ìˆ˜ì§‘**: ê´€ë ¨ taskë“¤ì— ì—°ê²°ëœ APIë“¤ì„ ìˆ˜ì§‘
5. **ì¤‘ë³µ ì œê±°**: ì¤‘ë³µëœ API ì œê±° í›„ ê²°ê³¼ ë°˜í™˜

#### GPT í”„ë¡¬í”„íŠ¸:
```
Given a user query and a list of task descriptions, identify the most relevant tasks.

Query: {query}

Task Descriptions:
{task_descriptions}

Return only the task names in order of relevance (most relevant first), separated by commas.
```

## ðŸ§ª ì‹¤í—˜ ì„¸íŒ…

### 1. ì •ëŸ‰ì  í‰ê°€ ì‹¤í—˜ (`quantitative_evaluation.py`)

#### ì‹¤í—˜ ì„¤ì •:
- **í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤**: 10ê°œ (ìˆ˜ë™ ìƒì„±)
- **í‰ê°€ ë©”íŠ¸ë¦­**: Precision, Recall, F1-Score
- **Ground Truth**: ìˆ˜ë™ìœ¼ë¡œ ì •ì˜ëœ ê´€ë ¨ API ëª©ë¡

#### í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì˜ˆì‹œ:
```python
test_cases = [
    {
        "query": "I want to listen to some music and get random facts. Can you help me find albums by a specific artist and also provide me with some interesting random facts?",
        "relevant_apis": [
            "Deezer:Album",
            "Deezer:Artist", 
            "Numbers:Get random fact",
            "Numbers:Get trivia fact"
        ]
    },
    {
        "query": "I'm looking for some humor to brighten my day. Can you fetch a random joke and also get me a funny quote?",
        "relevant_apis": [
            "Chuck Norris:/jokes/random",
            "Quotes:quote",
            "World of Jokes:Get Random Joke"
        ]
    }
]
```

#### í‰ê°€ í•¨ìˆ˜:
```python
def evaluate_retriever(test_cases):
    total_precision = 0
    total_recall = 0
    total_f1 = 0
    
    for test_case in test_cases:
        query = test_case["query"]
        ground_truth = set(test_case["relevant_apis"])
        
        # Retriever ì‹¤í–‰
        retrieved_apis = retriever.retrieve_apis_for_query(query)
        retrieved_set = set(retrieved_apis)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        precision = len(ground_truth & retrieved_set) / len(retrieved_set) if retrieved_set else 0
        recall = len(ground_truth & retrieved_set) / len(ground_truth) if ground_truth else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        total_precision += precision
        total_recall += recall
        total_f1 += f1
    
    return {
        'avg_precision': total_precision / len(test_cases),
        'avg_recall': total_recall / len(test_cases),
        'avg_f1': total_f1 / len(test_cases)
    }
```

### 2. ìƒì„¸ ë¶„ì„ ì‹¤í—˜ (`detailed_retriever_test.py`)

#### ì‹¤í—˜ ëª©ì :
- ë‹¨ê³„ë³„ retrieve ê³¼ì • ì‹œê°í™”
- GPT í”„ë¡¬í”„íŠ¸ì™€ ì‘ë‹µ í™•ì¸
- Task ë§¤ì¹­ ë° API ìˆ˜ì§‘ ê³¼ì • ë¶„ì„

#### ë¶„ì„ í”„ë¡œì„¸ìŠ¤:
1. **ì¿¼ë¦¬ ì „ì²˜ë¦¬**: ìž…ë ¥ ì¿¼ë¦¬ ì •ê·œí™”
2. **GPT í˜¸ì¶œ**: ëª¨ë“  task descriptionê³¼ í•¨ê»˜ GPT í˜¸ì¶œ
3. **ì‘ë‹µ íŒŒì‹±**: GPT ì‘ë‹µì—ì„œ ê´€ë ¨ task ì¶”ì¶œ
4. **API ìˆ˜ì§‘**: ê´€ë ¨ taskë“¤ì˜ API ìˆ˜ì§‘
5. **ê²°ê³¼ ë¶„ì„**: ê° ë‹¨ê³„ë³„ ê²°ê³¼ ì¶œë ¥

### 3. Ground Truth ë¹„êµ ì‹¤í—˜ (`compare_retriever_with_gt.py`)

#### ì‹¤í—˜ ì„¤ì •:
- **ë°ì´í„° ì†ŒìŠ¤**: ì‹¤ì œ ì¿¼ë¦¬ ë°ì´í„°ì—ì„œ ìƒ˜í”Œë§
- **Ground Truth**: ì‹¤ì œ ì¿¼ë¦¬ì— ì—°ê²°ëœ API ëª©ë¡
- **ë¹„êµ ë©”íŠ¸ë¦­**: ë§¤ì¹­/ëˆ„ë½/ì¶”ê°€ API ë¶„ì„

#### ë¹„êµ í”„ë¡œì„¸ìŠ¤:
```python
def compare_with_ground_truth(query_id, ground_truth_apis, retrieved_apis):
    ground_truth_set = set(ground_truth_apis)
    retrieved_set = set(retrieved_apis)
    
    matched = ground_truth_set & retrieved_set
    missed = ground_truth_set - retrieved_set
    extra = retrieved_set - ground_truth_set
    
    return {
        'matched': list(matched),
        'missed': list(missed),
        'extra': list(extra),
        'precision': len(matched) / len(retrieved_set) if retrieved_set else 0,
        'recall': len(matched) / len(ground_truth_set) if ground_truth_set else 0
    }
```

### 4. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í—˜ (`test_hybrid_retriever.py`)

#### ì‹¤í—˜ ì„¤ì •:
- **Vector Model**: all-MiniLM-L6-v2
- **LLM Model**: GPT-4o-mini
- **ê²€ìƒ‰ ë°©ì‹**: Vector similarity + LLM í•„í„°ë§
- **Top-k**: 10ê°œ ê²°ê³¼

#### ì‹¤í—˜ í”„ë¡œì„¸ìŠ¤:
1. **Vector Database êµ¬ì¶•**: API í…ìŠ¤íŠ¸ ìž„ë² ë”© ìƒì„±
2. **LLM í•„í„°ë§**: ì¿¼ë¦¬ ë¶„ì„ì„ í†µí•œ ì¹´í…Œê³ ë¦¬/íƒœìŠ¤í¬ í•„í„°ë§
3. **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰**: í•„í„°ë§ëœ ê²°ê³¼ì—ì„œ vector similarity ê²€ìƒ‰
4. **ì„±ëŠ¥ í‰ê°€**: Precision, Recall, F1-Score ê³„ì‚°

## ðŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
librarian/
â”œâ”€â”€ ðŸ“Š ë°ì´í„° íŒŒì¼
â”‚   â”œâ”€â”€ graph_info.json                    # Task-API ì—°ê²° ì •ë³´
â”‚   â”œâ”€â”€ task_descriptions.json             # ìƒì„±ëœ Task Descriptions
â”‚   â””â”€â”€ data/                             # ì¿¼ë¦¬ ë°ì´í„°
â”‚       â”œâ”€â”€ G1_query_sampled_50apis_enhanced.json
â”‚       â”œâ”€â”€ G2_query_sampled_50apis_enhanced.json
â”‚       â””â”€â”€ G3_query_sampled_50apis_enhanced.json
â”‚
â”œâ”€â”€ ðŸ”§ í•µì‹¬ ì»´í¬ë„ŒíŠ¸
â”‚   â”œâ”€â”€ task_description_generator.py      # Task Description ìƒì„±ê¸°
â”‚   â”œâ”€â”€ task_description_retriever.py     # Task Description ê¸°ë°˜ Retriever
â”‚   â””â”€â”€ knowledge_graph_builder.py        # ì§€ì‹ ê·¸ëž˜í”„ êµ¬ì¶•
â”‚
â”œâ”€â”€ ðŸ§ª ì‹¤í—˜ ë° í‰ê°€
â”‚   â”œâ”€â”€ test_task_retriever.py            # Retriever í…ŒìŠ¤íŠ¸
â”‚   â”œâ”€â”€ detailed_retriever_test.py        # ìƒì„¸ Retrieve ê³¼ì • ë¶„ì„
â”‚   â”œâ”€â”€ compare_retriever_with_gt.py      # Ground Truthì™€ ë¹„êµ
â”‚   â””â”€â”€ quantitative_evaluation.py        # ì •ëŸ‰ì  ì„±ëŠ¥ í‰ê°€
â”‚
â”œâ”€â”€ ðŸ“ˆ ë¶„ì„ ë„êµ¬
â”‚   â”œâ”€â”€ graph_connection_analyzer.py      # ê·¸ëž˜í”„ ì—°ê²° ë¶„ì„
â”‚   â”œâ”€â”€ graph_content_analyzer.py         # ê·¸ëž˜í”„ ë‚´ìš© ë¶„ì„
â”‚   â”œâ”€â”€ graph_visualizer.py               # ê·¸ëž˜í”„ ì‹œê°í™”
â”‚   â””â”€â”€ query_text_analyzer.py            # ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ë¶„ì„
â”‚
â””â”€â”€ ðŸ“š ë¬¸ì„œ
    â”œâ”€â”€ README.md                         # ì´ íŒŒì¼
    â””â”€â”€ README_task_description.md        # Task Description ìƒì„±ê¸° ë¬¸ì„œ
```

## ðŸŽ¯ ì£¼ìš” ê¸°ëŠ¥

### 1. Task Description ìƒì„±
- **íŒŒì¼**: `task_description_generator.py`
- **ê¸°ëŠ¥**: GPT-4o-minië¥¼ ì‚¬ìš©í•˜ì—¬ taskì˜ descriptionì„ ìžë™ ìƒì„±
- **ìž…ë ¥**: graph_info.jsonì˜ task-API ì—°ê²° ì •ë³´
- **ì¶œë ¥**: ê°„ê²°í•˜ê³  íŠ¹ì§•ì ì¸ task description (30-50ë‹¨ì–´)

### 2. Task Description ê¸°ë°˜ Retriever
- **íŒŒì¼**: `task_description_retriever.py`
- **ê¸°ëŠ¥**: ì¿¼ë¦¬ë¥¼ ë°›ì•„ ê´€ë ¨ taskë“¤ì„ ì°¾ê³  ì—°ê²°ëœ API ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
- **í”„ë¡œì„¸ìŠ¤**:
  1. ì¿¼ë¦¬ì™€ ëª¨ë“  task descriptionì„ GPTì— ì „ë‹¬
  2. ê´€ë ¨ taskë“¤ì„ ìˆœì„œëŒ€ë¡œ ë°˜í™˜ë°›ìŒ
  3. í•´ë‹¹ taskë“¤ì— ì—°ê²°ëœ APIë“¤ì„ ìˆ˜ì§‘
  4. ì¤‘ë³µ ì œê±° í›„ ê²°ê³¼ ë°˜í™˜

### 3. ì •ëŸ‰ì  ì„±ëŠ¥ í‰ê°€
- **íŒŒì¼**: `quantitative_evaluation.py`
- **ê¸°ëŠ¥**: Precision, Recall, F1-Scoreë¥¼ í†µí•œ ì„±ëŠ¥ ì¸¡ì •
- **ê²°ê³¼**: í˜„ìž¬ ì„±ëŠ¥ì€ ë§¤ìš° ë‚®ìŒ (F1-Score: 0.024)

## ðŸš€ ì‚¬ìš© ë°©ë²•

### 1. í™˜ê²½ ì„¤ì •
```bash
# OpenAI API í‚¤ ì„¤ì •
export OPENAI_API_KEY="your-api-key-here"

# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install openai sentence-transformers faiss-cpu networkx scikit-learn numpy
```

### 2. Task Description ìƒì„±
```bash
# ëª¨ë“  taskì˜ description ìƒì„±
python task_description_generator.py

# íŠ¹ì • taskì˜ description ìƒì„±
python task_description_generator.py merged_task_24
```

### 3. Retriever í…ŒìŠ¤íŠ¸
```bash
# ê¸°ë³¸ í…ŒìŠ¤íŠ¸
python test_task_retriever.py

# ìƒì„¸ ê³¼ì • ë¶„ì„
python detailed_retriever_test.py

# Ground Truthì™€ ë¹„êµ
python compare_retriever_with_gt.py

# ì •ëŸ‰ì  ì„±ëŠ¥ í‰ê°€
python quantitative_evaluation.py
```

## ðŸ“Š ì‹¤í—˜ ê²°ê³¼

### 1. Task Description ìƒì„± ê²°ê³¼
- **ìƒì„±ëœ Task**: 17ê°œ
- **Description íŠ¹ì„±**: ê°„ê²°í•˜ê³  í‚¤ì›Œë“œ ì¤‘ì‹¬ (30-50ë‹¨ì–´)
- **ì˜ˆì‹œ**:
  ```
  merged_task_24: "Integrates calendar invites and onboarding product queries via APIs. Key functionalities include sending native calendar invites, retrieving product categories, and checking order statuses."
  ```

### 2. Retriever ì„±ëŠ¥ í‰ê°€
- **í‰ê·  Precision**: 1.3% (ë§¤ìš° ë‚®ìŒ)
- **í‰ê·  Recall**: 20.0% (ë‚®ìŒ)
- **í‰ê·  F1-Score**: 0.024 (ë§¤ìš° ë‚®ìŒ)

### 3. ìƒì„¸ ì‹¤í—˜ ê²°ê³¼
- **í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤**: 10ê°œ
- **Ground Truth API ë§¤ì¹­**: ê±°ì˜ ì—†ìŒ
- **ì£¼ìš” ë¬¸ì œì **:
  - Task descriptionì´ ë„ˆë¬´ ì¼ë°˜ì 
  - GPT í”„ë¡¬í”„íŠ¸ ìµœì í™” í•„ìš”
  - API ë§¤ì¹­ ë¡œì§ ê°œì„  í•„ìš”

## ðŸ” ì‹¤í—˜ ê³¼ì •

### 1. ê¸°ë³¸ Retriever í…ŒìŠ¤íŠ¸
```bash
python test_task_retriever.py
```
- ì‹¤ì œ ë°ì´í„°ì—ì„œ 10ê°œ ì¿¼ë¦¬ ìƒ˜í”Œë§
- ê° ì¿¼ë¦¬ì— ëŒ€í•´ ê´€ë ¨ taskì™€ API ë°˜í™˜
- ê²°ê³¼ ìš”ì•½ ë° ë¶„ì„

### 2. ìƒì„¸ Retrieve ê³¼ì • ë¶„ì„
```bash
python detailed_retriever_test.py
```
- ë‹¨ê³„ë³„ retrieve ê³¼ì • ì‹œê°í™”
- GPT í”„ë¡¬í”„íŠ¸ì™€ ì‘ë‹µ í™•ì¸
- Task ë§¤ì¹­ ë° API ìˆ˜ì§‘ ê³¼ì • ë¶„ì„

### 3. Ground Truth ë¹„êµ
```bash
python compare_retriever_with_gt.py
```
- ì‹¤ì œ ì¿¼ë¦¬ì™€ ì •ë‹µ API ë¹„êµ
- ë§¤ì¹­/ëˆ„ë½/ì¶”ê°€ API ë¶„ì„
- ì •í™•ë„ ì¸¡ì •

### 4. ì •ëŸ‰ì  ì„±ëŠ¥ í‰ê°€
```bash
python quantitative_evaluation.py
```
- 10ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ ì„±ëŠ¥ ì¸¡ì •
- Precision, Recall, F1-Score ê³„ì‚°
- ì „ì²´ ì„±ëŠ¥ ìš”ì•½

## ðŸŽ¯ ì£¼ìš” ë°œê²¬ì‚¬í•­

### 1. Task Descriptionì˜ ì¤‘ìš”ì„±
- í˜„ìž¬ descriptionì´ ë„ˆë¬´ ì¼ë°˜ì 
- APIë³„ íŠ¹ì„±ì„ ë°˜ì˜í•˜ì§€ ëª»í•¨
- ë” êµ¬ì²´ì ì´ê³  íŠ¹ì§•ì ì¸ description í•„ìš”

### 2. GPT í”„ë¡¬í”„íŠ¸ ìµœì í™” í•„ìš”
- í˜„ìž¬ í”„ë¡¬í”„íŠ¸ë¡œëŠ” ì •í™•í•œ ë§¤ì¹­ ì–´ë ¤ì›€
- API ì´ë¦„ê³¼ ê¸°ëŠ¥ì„ ë” ëª…í™•ížˆ ì¸ì‹í•˜ë„ë¡ ê°œì„  í•„ìš”
- ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤ì¹­ ë„ìž… í•„ìš”

### 3. ì„±ëŠ¥ ê°œì„  ë°©í–¥
- Task description ìž¬ìƒì„± (ë” êµ¬ì²´ì )
- GPT í”„ë¡¬í”„íŠ¸ ìµœì í™”
- API ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ ê°œì„ 
- ë” ë§Žì€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ í™•ìž¥

## ðŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ

- **Python**: ì£¼ìš” í”„ë¡œê·¸ëž˜ë° ì–¸ì–´
- **OpenAI GPT-4o-mini**: Task description ìƒì„± ë° ì¿¼ë¦¬ ë§¤ì¹­
- **SentenceTransformer**: í…ìŠ¤íŠ¸ ìž„ë² ë”© ìƒì„±
- **FAISS**: ë²¡í„° ê²€ìƒ‰ ì¸ë±ìŠ¤
- **NetworkX**: ê·¸ëž˜í”„ ë¶„ì„ ë° ì‹œê°í™”
- **Scikit-learn**: í´ëŸ¬ìŠ¤í„°ë§ ë° ìœ ì‚¬ë„ ê³„ì‚°
- **JSON**: ë°ì´í„° ì €ìž¥ ë° êµí™˜
- **Graph Analysis**: Task-API ì—°ê²° ë¶„ì„

## ðŸ“ˆ í–¥í›„ ê°œì„  ê³„íš

1. **Task Description ê°œì„ **
   - APIë³„ íŠ¹ì„±ì„ ë°˜ì˜í•œ ë” êµ¬ì²´ì ì¸ description
   - í‚¤ì›Œë“œ ì¤‘ì‹¬ì˜ íŠ¹ì§•ì ì¸ description

2. **GPT í”„ë¡¬í”„íŠ¸ ìµœì í™”**
   - ë” ì •í™•í•œ task ë§¤ì¹­ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ íŠœë‹
   - API ì´ë¦„ê³¼ ê¸°ëŠ¥ ì¸ì‹ ê°œì„ 

3. **API ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜ ê°œì„ **
   - ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ ë§¤ì¹­
   - ë²¡í„° ìž„ë² ë”©ì„ í™œìš©í•œ ìœ ì‚¬ë„ ê³„ì‚°

4. **ì„±ëŠ¥ í‰ê°€ í™•ìž¥**
   - ë” ë§Žì€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
   - ë‹¤ì–‘í•œ ë„ë©”ì¸ì˜ ì¿¼ë¦¬
   - ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

## ðŸ“ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤.

## ðŸ¤ ê¸°ì—¬

ë²„ê·¸ ë¦¬í¬íŠ¸, ê¸°ëŠ¥ ì œì•ˆ, ì½”ë“œ ê¸°ì—¬ë¥¼ í™˜ì˜í•©ë‹ˆë‹¤!

---

**ì°¸ê³ **: ì´ í”„ë¡œì íŠ¸ëŠ” Task Description ê¸°ë°˜ API ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ í”„ë¡œí† íƒ€ìž…ì´ë©°, í˜„ìž¬ ì„±ëŠ¥ì€ ë§¤ìš° ë‚®ì€ ìƒíƒœìž…ë‹ˆë‹¤. ì§€ì†ì ì¸ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.
>>>>>>> 811af20... Add files via upload
