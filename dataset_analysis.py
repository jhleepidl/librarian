import json
import os
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from collections import defaultdict
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import pandas as pd

class DatasetAnalyzer:
    def __init__(self, model_path="ToolBench/ToolBench_IR_bert_based_uncased"):
        """ë°ì´í„°ì…‹ ë¶„ì„ì„ ìœ„í•œ í´ë˜ìŠ¤ ì´ˆê¸°í™”"""
        self.model = SentenceTransformer(model_path)
        self.api_embeddings = {}
        self.api_to_text = {}
        
    def extract_api_text(self, api):
        """API ì •ë³´ë¥¼ ToolBench inference ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        # ToolBench inferenceì—ì„œëŠ” tool_nameê³¼ api_nameë§Œ ì‚¬ìš©
        tool_name = api.get('tool_name', '') or ''
        api_name = api.get('api_name', '') or ''
        
        # ê³µë°±ìœ¼ë¡œ ì—°ê²° (ToolBench inference ë°©ì‹ê³¼ ë™ì¼)
        text = f"{tool_name} {api_name}"
        
        return text
    
    def get_api_embedding(self, api):
        """APIì˜ ì„ë² ë”©ì„ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒì„±"""
        api_text = self.extract_api_text(api)
        
        # API ì‹ë³„ì ìƒì„± (tool_name + api_name)
        api_id = f"{api.get('tool_name', '')}_{api.get('api_name', '')}"
        
        if api_id not in self.api_embeddings:
            self.api_embeddings[api_id] = self.model.encode(api_text)
            self.api_to_text[api_id] = api_text
            
        return api_id, self.api_embeddings[api_id]
    
    def analyze_file(self, file_path, global_api_pool):
        """ë‹¨ì¼ íŒŒì¼ ë¶„ì„"""
        print(f"ë¶„ì„ ì¤‘: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        results = {
            'total_queries': len(data),
            'queries_analyzed': 0,
            'avg_relevant_apis_per_query': 0,
            'similarity_stats': [],
            'clustering_results': [],
            'nearest_api_analysis': [],
            'api_embeddings': {}
        }
        
        all_relevant_apis = []
        all_apis_in_file = []  # íŒŒì¼ ë‚´ ëª¨ë“  API ìˆ˜ì§‘
        
        # ë¨¼ì € íŒŒì¼ ë‚´ ëª¨ë“  APIì˜ ì„ë² ë”©ì„ ìˆ˜ì§‘
        for query_data in data:
            for api in query_data.get('api_list', []):
                api_id, embedding = self.get_api_embedding(api)
                all_apis_in_file.append((api_id, embedding))
        
        # ì¤‘ë³µ ì œê±°
        unique_apis = {}
        for api_id, embedding in all_apis_in_file:
            if api_id not in unique_apis:
                unique_apis[api_id] = embedding
        
        for query_data in tqdm(data, desc="ì¿¼ë¦¬ ë¶„ì„ ì¤‘"):
            if 'relevant APIs' not in query_data or not query_data['relevant APIs']:
                continue
                
            relevant_apis = query_data['relevant APIs']
            results['queries_analyzed'] += 1
            
            # ê° relevant APIì˜ ì„ë² ë”© ìƒì„±
            api_embeddings = []
            api_ids = []
            
            for api_ref in relevant_apis:
                # api_refëŠ” [tool_name, api_name] í˜•íƒœ
                if len(api_ref) != 2:
                    continue
                    
                tool_name, api_name = api_ref
                
                # api_listì—ì„œ í•´ë‹¹ API ì°¾ê¸°
                target_api = None
                for api in query_data.get('api_list', []):
                    if api.get('tool_name') == tool_name and api.get('api_name') == api_name:
                        target_api = api
                        break
                
                if target_api is None:
                    continue
                    
                api_id, embedding = self.get_api_embedding(target_api)
                api_embeddings.append(embedding)
                api_ids.append(api_id)
                all_relevant_apis.append(api_id)
            
            if len(api_embeddings) < 2:
                continue
                
            # ì„ë² ë”©ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
            embeddings_array = np.array(api_embeddings)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarity_matrix = cosine_similarity(embeddings_array)
            
            # ëŒ€ê°ì„ ì„ ì œì™¸í•œ ìœ ì‚¬ë„ë§Œ ê³„ì‚° (ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„ ì œì™¸)
            # ìƒì‚¼ê° í–‰ë ¬ì—ì„œ ëŒ€ê°ì„ ì„ ì œì™¸í•œ ê°’ë“¤ë§Œ ì¶”ì¶œ
            upper_triangle = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]
            
            # í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
            avg_similarity = np.mean(upper_triangle)
            min_similarity = np.min(upper_triangle)
            max_similarity = np.max(upper_triangle)
            
            results['similarity_stats'].append({
                'query_id': query_data.get('query_id', 'unknown'),
                'num_apis': len(api_embeddings),
                'avg_similarity': avg_similarity,
                'min_similarity': min_similarity,
                'max_similarity': max_similarity,
                'api_ids': api_ids
            })
            
            # Clustering ë¶„ì„
            if len(api_embeddings) >= 3:
                # K-means í´ëŸ¬ìŠ¤í„°ë§ (í´ëŸ¬ìŠ¤í„° ìˆ˜ëŠ” API ê°œìˆ˜ì˜ ì ˆë°˜ìœ¼ë¡œ ì„¤ì •)
                n_clusters = min(len(api_embeddings) // 2, len(api_embeddings) - 1)
                if n_clusters >= 2:
                    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
                    cluster_labels = kmeans.fit_predict(embeddings_array)
                    
                    # í´ëŸ¬ìŠ¤í„° ë‚´ í‰ê·  ê±°ë¦¬ ê³„ì‚°
                    cluster_distances = []
                    for i in range(n_clusters):
                        cluster_indices = np.where(cluster_labels == i)[0]
                        if len(cluster_indices) > 1:
                            cluster_embeddings = embeddings_array[cluster_indices]
                            cluster_similarity = cosine_similarity(cluster_embeddings)
                            np.fill_diagonal(cluster_similarity, 0)
                            cluster_distances.append(np.mean(cluster_similarity))
                    
                    results['clustering_results'].append({
                        'query_id': query_data.get('query_id', 'unknown'),
                        'num_clusters': n_clusters,
                        'cluster_labels': cluster_labels.tolist(),
                        'avg_cluster_similarity': np.mean(cluster_distances) if cluster_distances else 0,
                        'api_ids': api_ids
                    })
            
            # Relevant APIë“¤ì˜ í‰ê·  ì„ë² ë”©ê³¼ ê°€ì¥ ê°€ê¹Œìš´ API ì°¾ê¸°
            if len(api_embeddings) >= 1:
                # Relevant APIë“¤ì˜ í‰ê·  ì„ë² ë”© ê³„ì‚°
                relevant_avg_embedding = np.mean(embeddings_array, axis=0)
                
                # ëª¨ë“  APIì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
                similarities_with_all = []
                for api_id, api_embedding in global_api_pool.items():
                    similarity = cosine_similarity([relevant_avg_embedding], [api_embedding])[0][0]
                    similarities_with_all.append((api_id, similarity))
                
                # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
                similarities_with_all.sort(key=lambda x: x[1], reverse=True)
                
                # Top 5 APIì™€ relevant APIë“¤ì˜ ìœ ì‚¬ë„ ë¶„ì„
                top_5_apis = similarities_with_all[:5]
                relevant_api_similarities = []
                
                for relevant_api_id in api_ids:
                    for api_id, similarity in similarities_with_all:
                        if api_id == relevant_api_id:
                            relevant_api_similarities.append(similarity)
                            break
                
                # Relevant APIë“¤ì˜ í‰ê·  ìœ ì‚¬ë„
                avg_relevant_similarity = np.mean(relevant_api_similarities) if relevant_api_similarities else 0
                
                # Top 1 APIê°€ relevant APIì¸ì§€ í™•ì¸
                top_1_is_relevant = similarities_with_all[0][0] in api_ids
                
                # Top 5 ì¤‘ relevant API ê°œìˆ˜
                top_5_relevant_count = sum(1 for api_id, _ in top_5_apis if api_id in api_ids)
                
                results['nearest_api_analysis'].append({
                    'query_id': query_data.get('query_id', 'unknown'),
                    'num_relevant_apis': len(api_embeddings),
                    'top_5_apis': top_5_apis,
                    'avg_relevant_similarity': avg_relevant_similarity,
                    'top_1_is_relevant': top_1_is_relevant,
                    'top_5_relevant_count': top_5_relevant_count,
                    'relevant_api_ids': api_ids
                })
        
        # ì „ì²´ í†µê³„ ê³„ì‚°
        if results['similarity_stats']:
            results['avg_relevant_apis_per_query'] = np.mean([stat['num_apis'] for stat in results['similarity_stats']])
            results['avg_similarity_across_queries'] = np.mean([stat['avg_similarity'] for stat in results['similarity_stats']])
            results['min_similarity_across_queries'] = np.min([stat['min_similarity'] for stat in results['similarity_stats']])
            results['max_similarity_across_queries'] = np.max([stat['max_similarity'] for stat in results['similarity_stats']])
        
        return results
    
    def analyze_all_files(self, data_dir):
        """ëª¨ë“  íŒŒì¼ ë¶„ì„"""
        files = [f for f in os.listdir(data_dir) if f.endswith('.json')]
        all_results = {}
        
        # ë¨¼ì € ëª¨ë“  íŒŒì¼ì—ì„œ APIë“¤ì„ ìˆ˜ì§‘í•˜ì—¬ ì „ì—­ API í’€ ìƒì„±
        print("ì „ì—­ API í’€ì„ ìƒì„±í•˜ëŠ” ì¤‘...")
        global_api_pool = {}
        
        for file in tqdm(files, desc="ì „ì—­ API í’€ ìƒì„±"):
            file_path = os.path.join(data_dir, file)
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            for query_data in data:
                for api in query_data.get('api_list', []):
                    api_id, embedding = self.get_api_embedding(api)
                    if api_id not in global_api_pool:
                        global_api_pool[api_id] = embedding
        
        print(f"ì „ì—­ API í’€ í¬ê¸°: {len(global_api_pool)}")
        
        for file in files:
            file_path = os.path.join(data_dir, file)
            results = self.analyze_file(file_path, global_api_pool)
            all_results[file] = results
        
        # ì „ì—­ API í’€ í¬ê¸°ë¥¼ ê²°ê³¼ì— ì¶”ê°€
        all_results['_global_api_pool_size'] = len(global_api_pool)
            
        return all_results
    
    def analyze_partial_file(self, file_path, max_queries=100, global_api_pool=None):
        """ëŒ€ìš©ëŸ‰ íŒŒì¼ì˜ ì¼ë¶€ë§Œ ë¶„ì„"""
        print(f"ë¶„ì„ ì¤‘: {file_path} (ìµœëŒ€ {max_queries}ê°œ ì¿¼ë¦¬)")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ì¼ë¶€ë§Œ ì‚¬ìš©
        if len(data) > max_queries:
            print(f"ì „ì²´ {len(data)}ê°œ ì¤‘ {max_queries}ê°œë§Œ ì‚¬ìš©")
            data = data[:max_queries]
        
        results = {
            'total_queries': len(data),
            'queries_analyzed': 0,
            'avg_relevant_apis_per_query': 0,
            'similarity_stats': [],
            'clustering_results': [],
            'nearest_api_analysis': [],
            'api_embeddings': {}
        }
        
        all_relevant_apis = []
        all_apis_in_file = []  # íŒŒì¼ ë‚´ ëª¨ë“  API ìˆ˜ì§‘
        
        # ë¨¼ì € íŒŒì¼ ë‚´ ëª¨ë“  APIì˜ ì„ë² ë”©ì„ ìˆ˜ì§‘
        for query_data in data:
            for api in query_data.get('relevant_apis', []):
                api_id, embedding = self.get_api_embedding(api)
                all_apis_in_file.append((api_id, embedding))
        
        # ì¤‘ë³µ ì œê±°
        unique_apis = {}
        for api_id, embedding in all_apis_in_file:
            if api_id not in unique_apis:
                unique_apis[api_id] = embedding
        
        for query_data in tqdm(data, desc="ì¿¼ë¦¬ ë¶„ì„ ì¤‘"):
            if 'relevant_apis' not in query_data or not query_data['relevant_apis']:
                continue
                
            relevant_apis = query_data['relevant_apis']
            results['queries_analyzed'] += 1
            
            # ê° relevant APIì˜ ì„ë² ë”© ìƒì„±
            api_embeddings = []
            api_ids = []
            
            for api in relevant_apis:
                api_id, embedding = self.get_api_embedding(api)
                api_embeddings.append(embedding)
                api_ids.append(api_id)
                all_relevant_apis.append(api_id)
            
            if len(api_embeddings) < 2:
                continue
                
            # ì„ë² ë”©ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
            embeddings_array = np.array(api_embeddings)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarity_matrix = cosine_similarity(embeddings_array)
            
            # ëŒ€ê°ì„ ì„ ì œì™¸í•œ ìœ ì‚¬ë„ë§Œ ê³„ì‚° (ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„ ì œì™¸)
            # ìƒì‚¼ê° í–‰ë ¬ì—ì„œ ëŒ€ê°ì„ ì„ ì œì™¸í•œ ê°’ë“¤ë§Œ ì¶”ì¶œ
            upper_triangle = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]
            
            # í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
            avg_similarity = np.mean(upper_triangle)
            min_similarity = np.min(upper_triangle)
            max_similarity = np.max(upper_triangle)
            
            results['similarity_stats'].append({
                'query_id': query_data.get('query_id', 'unknown'),
                'num_apis': len(api_embeddings),
                'avg_similarity': avg_similarity,
                'min_similarity': min_similarity,
                'max_similarity': max_similarity,
                'api_ids': api_ids
            })
            
            # Clustering ë¶„ì„
            if len(api_embeddings) >= 3:
                # K-means í´ëŸ¬ìŠ¤í„°ë§ (í´ëŸ¬ìŠ¤í„° ìˆ˜ëŠ” API ê°œìˆ˜ì˜ ì ˆë°˜ìœ¼ë¡œ ì„¤ì •)
                n_clusters = min(len(api_embeddings) // 2, len(api_embeddings) - 1)
                if n_clusters >= 2:
                    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                    cluster_labels = kmeans.fit_predict(embeddings_array)
                    
                    # í´ëŸ¬ìŠ¤í„° ë‚´ í‰ê·  ê±°ë¦¬ ê³„ì‚°
                    cluster_distances = []
                    for i in range(n_clusters):
                        cluster_indices = np.where(cluster_labels == i)[0]
                        if len(cluster_indices) > 1:
                            cluster_embeddings = embeddings_array[cluster_indices]
                            cluster_similarity = cosine_similarity(cluster_embeddings)
                            np.fill_diagonal(cluster_similarity, 0)
                            cluster_distances.append(np.mean(cluster_similarity))
                    
                    results['clustering_results'].append({
                        'query_id': query_data.get('query_id', 'unknown'),
                        'num_clusters': n_clusters,
                        'cluster_labels': cluster_labels.tolist(),
                        'avg_cluster_similarity': np.mean(cluster_distances) if cluster_distances else 0,
                        'api_ids': api_ids
                    })
            
            # Relevant APIë“¤ì˜ í‰ê·  ì„ë² ë”©ê³¼ ê°€ì¥ ê°€ê¹Œìš´ API ì°¾ê¸°
            if len(api_embeddings) >= 1 and global_api_pool:
                # Relevant APIë“¤ì˜ í‰ê·  ì„ë² ë”© ê³„ì‚°
                relevant_avg_embedding = np.mean(embeddings_array, axis=0)
                
                # ëª¨ë“  APIì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
                similarities_with_all = []
                for api_id, api_embedding in global_api_pool.items():
                    similarity = cosine_similarity([relevant_avg_embedding], [api_embedding])[0][0]
                    similarities_with_all.append((api_id, similarity))
                
                # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
                similarities_with_all.sort(key=lambda x: x[1], reverse=True)
                
                # Top 5 APIì™€ relevant APIë“¤ì˜ ìœ ì‚¬ë„ ë¶„ì„
                top_5_apis = similarities_with_all[:5]
                relevant_api_similarities = []
                
                for relevant_api_id in api_ids:
                    for api_id, similarity in similarities_with_all:
                        if api_id == relevant_api_id:
                            relevant_api_similarities.append(similarity)
                            break
                
                # Relevant APIë“¤ì˜ í‰ê·  ìœ ì‚¬ë„
                avg_relevant_similarity = np.mean(relevant_api_similarities) if relevant_api_similarities else 0
                
                # Top 1 APIê°€ relevant APIì¸ì§€ í™•ì¸
                top_1_is_relevant = similarities_with_all[0][0] in api_ids
                
                # Top 5 ì¤‘ relevant API ê°œìˆ˜
                top_5_relevant_count = sum(1 for api_id, _ in top_5_apis if api_id in api_ids)
                
                results['nearest_api_analysis'].append({
                    'query_id': query_data.get('query_id', 'unknown'),
                    'num_relevant_apis': len(api_embeddings),
                    'top_5_apis': top_5_apis,
                    'avg_relevant_similarity': avg_relevant_similarity,
                    'top_1_is_relevant': top_1_is_relevant,
                    'top_5_relevant_count': top_5_relevant_count,
                    'relevant_api_ids': api_ids
                })
        
        # ì „ì²´ í†µê³„ ê³„ì‚°
        if results['similarity_stats']:
            results['avg_relevant_apis_per_query'] = np.mean([stat['num_apis'] for stat in results['similarity_stats']])
            results['avg_similarity_across_queries'] = np.mean([stat['avg_similarity'] for stat in results['similarity_stats']])
            results['min_similarity_across_queries'] = np.min([stat['min_similarity'] for stat in results['similarity_stats']])
            results['max_similarity_across_queries'] = np.max([stat['max_similarity'] for stat in results['similarity_stats']])
        
        return results
    
    def analyze_partial_file_g2(self, file_path, max_queries=100, global_api_pool=None):
        """G2_query.json í˜•ì‹ì˜ ëŒ€ìš©ëŸ‰ íŒŒì¼ ë¶„ì„ (relevant APIsê°€ ì—†ëŠ” ê²½ìš°)"""
        print(f"ë¶„ì„ ì¤‘: {file_path} (ìµœëŒ€ {max_queries}ê°œ ì¿¼ë¦¬)")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ì¼ë¶€ë§Œ ì‚¬ìš©
        if len(data) > max_queries:
            print(f"ì „ì²´ {len(data)}ê°œ ì¤‘ {max_queries}ê°œë§Œ ì‚¬ìš©")
            data = data[:max_queries]
        
        results = {
            'total_queries': len(data),
            'queries_analyzed': 0,
            'avg_relevant_apis_per_query': 0,
            'similarity_stats': [],
            'clustering_results': [],
            'nearest_api_analysis': [],
            'api_embeddings': {}
        }
        
        all_relevant_apis = []
        all_apis_in_file = []  # íŒŒì¼ ë‚´ ëª¨ë“  API ìˆ˜ì§‘
        
        # ë¨¼ì € íŒŒì¼ ë‚´ ëª¨ë“  APIì˜ ì„ë² ë”©ì„ ìˆ˜ì§‘
        for query_data in data:
            for api in query_data.get('api_list', []):
                api_id, embedding = self.get_api_embedding(api)
                all_apis_in_file.append((api_id, embedding))
        
        # ì¤‘ë³µ ì œê±°
        unique_apis = {}
        for api_id, embedding in all_apis_in_file:
            if api_id not in unique_apis:
                unique_apis[api_id] = embedding
        
        for query_data in tqdm(data, desc="ì¿¼ë¦¬ ë¶„ì„ ì¤‘"):
            # G2_query.jsonì€ relevant APIsê°€ ì—†ìœ¼ë¯€ë¡œ api_listì˜ ëª¨ë“  APIë¥¼ relevantë¡œ ê°„ì£¼
            relevant_apis = query_data.get('api_list', [])
            if not relevant_apis:
                continue
                
            results['queries_analyzed'] += 1
            
            # ê° relevant APIì˜ ì„ë² ë”© ìƒì„±
            api_embeddings = []
            api_ids = []
            
            for api in relevant_apis:
                api_id, embedding = self.get_api_embedding(api)
                api_embeddings.append(embedding)
                api_ids.append(api_id)
                all_relevant_apis.append(api_id)
            
            if len(api_embeddings) < 2:
                continue
                
            # ì„ë² ë”©ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
            embeddings_array = np.array(api_embeddings)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarity_matrix = cosine_similarity(embeddings_array)
            
            # ëŒ€ê°ì„ ì„ ì œì™¸í•œ ìœ ì‚¬ë„ë§Œ ê³„ì‚° (ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„ ì œì™¸)
            # ìƒì‚¼ê° í–‰ë ¬ì—ì„œ ëŒ€ê°ì„ ì„ ì œì™¸í•œ ê°’ë“¤ë§Œ ì¶”ì¶œ
            upper_triangle = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]
            
            # í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
            avg_similarity = np.mean(upper_triangle)
            min_similarity = np.min(upper_triangle)
            max_similarity = np.max(upper_triangle)
            
            results['similarity_stats'].append({
                'query_id': query_data.get('query_id', 'unknown'),
                'num_apis': len(api_embeddings),
                'avg_similarity': avg_similarity,
                'min_similarity': min_similarity,
                'max_similarity': max_similarity,
                'api_ids': api_ids
            })
            
            # Clustering ë¶„ì„
            if len(api_embeddings) >= 3:
                # K-means í´ëŸ¬ìŠ¤í„°ë§ (í´ëŸ¬ìŠ¤í„° ìˆ˜ëŠ” API ê°œìˆ˜ì˜ ì ˆë°˜ìœ¼ë¡œ ì„¤ì •)
                n_clusters = min(len(api_embeddings) // 2, len(api_embeddings) - 1)
                if n_clusters >= 2:
                    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                    cluster_labels = kmeans.fit_predict(embeddings_array)
                    
                    # í´ëŸ¬ìŠ¤í„° ë‚´ í‰ê·  ê±°ë¦¬ ê³„ì‚°
                    cluster_distances = []
                    for i in range(n_clusters):
                        cluster_indices = np.where(cluster_labels == i)[0]
                        if len(cluster_indices) > 1:
                            cluster_embeddings = embeddings_array[cluster_indices]
                            cluster_similarity = cosine_similarity(cluster_embeddings)
                            np.fill_diagonal(cluster_similarity, 0)
                            cluster_distances.append(np.mean(cluster_similarity))
                    
                    results['clustering_results'].append({
                        'query_id': query_data.get('query_id', 'unknown'),
                        'num_clusters': n_clusters,
                        'cluster_labels': cluster_labels.tolist(),
                        'avg_cluster_similarity': np.mean(cluster_distances) if cluster_distances else 0,
                        'api_ids': api_ids
                    })
            
            # Relevant APIë“¤ì˜ í‰ê·  ì„ë² ë”©ê³¼ ê°€ì¥ ê°€ê¹Œìš´ API ì°¾ê¸°
            if len(api_embeddings) >= 1 and global_api_pool:
                # Relevant APIë“¤ì˜ í‰ê·  ì„ë² ë”© ê³„ì‚°
                relevant_avg_embedding = np.mean(embeddings_array, axis=0)
                
                # ëª¨ë“  APIì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
                similarities_with_all = []
                for api_id, api_embedding in global_api_pool.items():
                    similarity = cosine_similarity([relevant_avg_embedding], [api_embedding])[0][0]
                    similarities_with_all.append((api_id, similarity))
                
                # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
                similarities_with_all.sort(key=lambda x: x[1], reverse=True)
                
                # Top 5 APIì™€ relevant APIë“¤ì˜ ìœ ì‚¬ë„ ë¶„ì„
                top_5_apis = similarities_with_all[:5]
                relevant_api_similarities = []
                
                for relevant_api_id in api_ids:
                    for api_id, similarity in similarities_with_all:
                        if api_id == relevant_api_id:
                            relevant_api_similarities.append(similarity)
                            break
                
                # Relevant APIë“¤ì˜ í‰ê·  ìœ ì‚¬ë„
                avg_relevant_similarity = np.mean(relevant_api_similarities) if relevant_api_similarities else 0
                
                # Top 1 APIê°€ relevant APIì¸ì§€ í™•ì¸
                top_1_is_relevant = similarities_with_all[0][0] in api_ids
                
                # Top 5 ì¤‘ relevant API ê°œìˆ˜
                top_5_relevant_count = sum(1 for api_id, _ in top_5_apis if api_id in api_ids)
                
                results['nearest_api_analysis'].append({
                    'query_id': query_data.get('query_id', 'unknown'),
                    'num_relevant_apis': len(api_embeddings),
                    'top_5_apis': top_5_apis,
                    'avg_relevant_similarity': avg_relevant_similarity,
                    'top_1_is_relevant': top_1_is_relevant,
                    'top_5_relevant_count': top_5_relevant_count,
                    'relevant_api_ids': api_ids
                })
        
        # ì „ì²´ í†µê³„ ê³„ì‚°
        if results['similarity_stats']:
            results['avg_relevant_apis_per_query'] = np.mean([stat['num_apis'] for stat in results['similarity_stats']])
            results['avg_similarity_across_queries'] = np.mean([stat['avg_similarity'] for stat in results['similarity_stats']])
            results['min_similarity_across_queries'] = np.min([stat['min_similarity'] for stat in results['similarity_stats']])
            results['max_similarity_across_queries'] = np.max([stat['max_similarity'] for stat in results['similarity_stats']])
        
        return results
    
    def generate_report(self, results):
        """ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = []
        report.append("=" * 60)
        report.append("ë°ì´í„°ì…‹ íŠ¹ì„± ë¶„ì„ ë¦¬í¬íŠ¸ (ì „ì—­ API í’€ ì‚¬ìš©)")
        report.append("=" * 60)
        
        # ì „ì—­ API í’€ í¬ê¸° ê°€ì ¸ì˜¤ê¸°
        global_api_count = 0
        for filename, result in results.items():
            if filename != '_global_api_pool_size' and '_global_api_pool_size' in result:
                global_api_count = result['_global_api_pool_size']
                break
        
        report.append(f"ì „ì—­ API í’€ í¬ê¸°: {global_api_count}ê°œ")
        report.append("=" * 60)
        
        for filename, result in results.items():
            # ì „ì—­ API í’€ í¬ê¸° ì •ë³´ëŠ” ê±´ë„ˆë›°ê¸°
            if filename == '_global_api_pool_size':
                continue
                
            report.append(f"\nğŸ“ íŒŒì¼: {filename}")
            report.append("-" * 40)
            report.append(f"ì´ ì¿¼ë¦¬ ìˆ˜: {result['total_queries']}")
            report.append(f"ë¶„ì„ëœ ì¿¼ë¦¬ ìˆ˜: {result['queries_analyzed']}")
            report.append(f"ì¿¼ë¦¬ë‹¹ í‰ê·  relevant API ìˆ˜: {result.get('avg_relevant_apis_per_query', 0):.2f}")
            
            if result['similarity_stats']:
                report.append(f"ì „ì²´ í‰ê·  ìœ ì‚¬ë„: {result.get('avg_similarity_across_queries', 0):.4f}")
                report.append(f"ìµœì†Œ ìœ ì‚¬ë„: {result.get('min_similarity_across_queries', 0):.4f}")
                report.append(f"ìµœëŒ€ ìœ ì‚¬ë„: {result.get('max_similarity_across_queries', 0):.4f}")
                
                # ìœ ì‚¬ë„ ë¶„í¬
                similarities = [stat['avg_similarity'] for stat in result['similarity_stats']]
                report.append(f"ìœ ì‚¬ë„ í‘œì¤€í¸ì°¨: {np.std(similarities):.4f}")
                
                # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼
                if result['clustering_results']:
                    cluster_similarities = [cr['avg_cluster_similarity'] for cr in result['clustering_results']]
                    report.append(f"í´ëŸ¬ìŠ¤í„°ë§ëœ ì¿¼ë¦¬ ìˆ˜: {len(result['clustering_results'])}")
                    report.append(f"í‰ê·  í´ëŸ¬ìŠ¤í„° ë‚´ ìœ ì‚¬ë„: {np.mean(cluster_similarities):.4f}")
                
                # Nearest API ë¶„ì„ ê²°ê³¼ (ì „ì—­ í’€ ì‚¬ìš©)
                if result['nearest_api_analysis']:
                    top_1_relevant_count = sum(1 for na in result['nearest_api_analysis'] if na['top_1_is_relevant'])
                    top_5_relevant_counts = [na['top_5_relevant_count'] for na in result['nearest_api_analysis']]
                    avg_relevant_similarities = [na['avg_relevant_similarity'] for na in result['nearest_api_analysis']]
                    
                    report.append(f"Nearest API ë¶„ì„ (ì „ì—­ API í’€ ì‚¬ìš©):")
                    report.append(f"  - Top 1ì´ relevantì¸ ì¿¼ë¦¬ ìˆ˜: {top_1_relevant_count}/{len(result['nearest_api_analysis'])} ({top_1_relevant_count/len(result['nearest_api_analysis'])*100:.1f}%)")
                    report.append(f"  - Top 5 ì¤‘ relevant í‰ê·  ê°œìˆ˜: {np.mean(top_5_relevant_counts):.2f}")
                    report.append(f"  - Relevant APIë“¤ì˜ í‰ê·  ìœ ì‚¬ë„: {np.mean(avg_relevant_similarities):.4f}")
                    report.append(f"  - ê²€ìƒ‰ ë‚œì´ë„: ì „ì—­ {global_api_count}ê°œ API ì¤‘ì—ì„œ ì •í™•í•œ API ì°¾ê¸°")
        
        return "\n".join(report)
    
    def save_detailed_results(self, results, output_dir="analysis_results"):
        """ìƒì„¸ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        os.makedirs(output_dir, exist_ok=True)
        
        def convert_to_serializable(obj):
            """ì¬ê·€ì ìœ¼ë¡œ ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, (np.float32, np.float64, np.float16)):
                return float(obj)
            elif isinstance(obj, (np.int32, np.int64, np.int16, np.int8)):
                return int(obj)
            elif isinstance(obj, dict):
                return {k: convert_to_serializable(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_to_serializable(item) for item in obj]
            elif isinstance(obj, tuple):
                return [convert_to_serializable(item) for item in obj]
            else:
                return obj
        
        for filename, result in results.items():
            output_file = os.path.join(output_dir, f"{filename.replace('.json', '_analysis.json')}")
            
            # ì¬ê·€ì ìœ¼ë¡œ ëª¨ë“  numpy íƒ€ì…ì„ ë³€í™˜
            serializable_result = convert_to_serializable(result)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_result, f, indent=2, ensure_ascii=False)
        
        print(f"ìƒì„¸ ê²°ê³¼ê°€ {output_dir} ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ë°ì´í„°ì…‹ íŠ¹ì„± ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = DatasetAnalyzer()
    
    # G3_query.json íŒŒì¼ ë¶„ì„
    g3_query_file = "/home/jhlee/ToolBench/data/instruction/G3_query.json"
    
    if os.path.exists(g3_query_file):
        print(f"\n{g3_query_file} íŒŒì¼ì„ ë¶„ì„í•©ë‹ˆë‹¤...")
        
        # ë¨¼ì € íŒŒì¼ í¬ê¸° í™•ì¸
        file_size = os.path.getsize(g3_query_file) / (1024 * 1024)  # MB
        print(f"íŒŒì¼ í¬ê¸°: {file_size:.1f} MB")
        
        # ì „ì—­ API í’€ ìƒì„± (ì „ì²´ íŒŒì¼ì—ì„œ API ìˆ˜ì§‘)
        print("ì „ì—­ API í’€ì„ ìƒì„±í•˜ëŠ” ì¤‘...")
        global_api_pool = {}
        
        with open(g3_query_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ì „ì²´ ë°ì´í„°ì—ì„œ API ìˆ˜ì§‘ (ìµœëŒ€ 1000ê°œ ì¿¼ë¦¬ë§Œ ì‚¬ìš©)
        max_queries_for_pool = min(1000, len(data))
        for i in range(max_queries_for_pool):
            query_data = data[i]
            for api in query_data.get('api_list', []):
                api_id, embedding = analyzer.get_api_embedding(api)
                if api_id not in global_api_pool:
                    global_api_pool[api_id] = embedding
        
        print(f"ì „ì—­ API í’€ í¬ê¸°: {len(global_api_pool)}ê°œ")
        
        # ì¼ë¶€ë§Œ ë¶„ì„ (100ê°œ ì¿¼ë¦¬)
        results = analyzer.analyze_partial_file_g2(g3_query_file, max_queries=100, global_api_pool=global_api_pool)
        
        # ì „ì—­ API í’€ í¬ê¸°ë¥¼ ê²°ê³¼ì— ì¶”ê°€
        results['_global_api_pool_size'] = len(global_api_pool)
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = analyzer.generate_report({'g3_query_partial': results})
        print(report)
        
        # ìƒì„¸ ê²°ê³¼ ì €ì¥
        analyzer.save_detailed_results({'g3_query_partial': results}, "analysis_results_g3")
        
        # ë¦¬í¬íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        with open("g3_query_analysis_report.txt", "w", encoding="utf-8") as f:
            f.write(report)
        
        print("\në¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ë¦¬í¬íŠ¸ê°€ g3_query_analysis_report.txt íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {g3_query_file}")
        
        # train.json íŒŒì¼ ë¶„ì„ (ì¼ë¶€ë§Œ)
        train_file = "librarian/data/train.json"
        
        if os.path.exists(train_file):
            print(f"\n{train_file} íŒŒì¼ì„ ë¶„ì„í•©ë‹ˆë‹¤...")
            
            # ë¨¼ì € íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = os.path.getsize(train_file) / (1024 * 1024)  # MB
            print(f"íŒŒì¼ í¬ê¸°: {file_size:.1f} MB")
            
            # ì „ì—­ API í’€ ìƒì„± (ì „ì²´ íŒŒì¼ì—ì„œ API ìˆ˜ì§‘)
            print("ì „ì—­ API í’€ì„ ìƒì„±í•˜ëŠ” ì¤‘...")
            global_api_pool = {}
            
            with open(train_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ì „ì²´ ë°ì´í„°ì—ì„œ API ìˆ˜ì§‘ (ìµœëŒ€ 1000ê°œ ì¿¼ë¦¬ë§Œ ì‚¬ìš©)
            max_queries_for_pool = min(1000, len(data))
            for i in range(max_queries_for_pool):
                query_data = data[i]
                for api in query_data.get('relevant_apis', []):
                    api_id, embedding = analyzer.get_api_embedding(api)
                    if api_id not in global_api_pool:
                        global_api_pool[api_id] = embedding
            
            print(f"ì „ì—­ API í’€ í¬ê¸°: {len(global_api_pool)}ê°œ")
            
            # ì¼ë¶€ë§Œ ë¶„ì„ (100ê°œ ì¿¼ë¦¬)
            results = analyzer.analyze_partial_file(train_file, max_queries=100, global_api_pool=global_api_pool)
            
            # ì „ì—­ API í’€ í¬ê¸°ë¥¼ ê²°ê³¼ì— ì¶”ê°€
            results['_global_api_pool_size'] = len(global_api_pool)
            
            # ë¦¬í¬íŠ¸ ìƒì„±
            report = analyzer.generate_report({'train_partial': results})
            print(report)
            
            # ìƒì„¸ ê²°ê³¼ ì €ì¥
            analyzer.save_detailed_results({'train_partial': results}, "analysis_results_train")
            
            # ë¦¬í¬íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥
            with open("train_analysis_report.txt", "w", encoding="utf-8") as f:
                f.write(report)
            
            print("\në¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            print("ë¦¬í¬íŠ¸ê°€ train_analysis_report.txt íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_file}")
            
            # ê¸°ì¡´ ToolBench ë°ì´í„° ë¶„ì„
            data_dir = "/home/jhlee/ToolBench/data/test_instruction"
            
            print(f"\nê¸°ì¡´ ToolBench ë°ì´í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤: {data_dir}")
            
            # ëª¨ë“  íŒŒì¼ ë¶„ì„
            results = analyzer.analyze_all_files(data_dir)
            
            # ë¦¬í¬íŠ¸ ìƒì„±
            report = analyzer.generate_report(results)
            print(report)
            
            # ìƒì„¸ ê²°ê³¼ ì €ì¥
            analyzer.save_detailed_results(results)
            
            # ë¦¬í¬íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥
            with open("dataset_analysis_report.txt", "w", encoding="utf-8") as f:
                f.write(report)
            
            print("\në¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            print("ë¦¬í¬íŠ¸ê°€ dataset_analysis_report.txt íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 